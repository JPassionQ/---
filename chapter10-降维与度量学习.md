## Chapter10 降维与度量学习

### k近邻学习

> k近邻（KNN）学习是一种常用的监督学习方法，其工作机制为：给定测试样本，基于某种距离度量找出训练集中预期最靠近的k个训练样本，然后基于这k个**邻居**的信息进行预测

* 在**分类**任务中可使用**投票法**：选择这k个样本中出现最多的类别标记作为预测结果
* 在**回归**任务中可使用**平均法**：即将这k个样本的实值输出标记的平均值作为预测结果
* 还可基于距离远近进行加权平均或加权投票，距离越近的样本权重最大

**懒惰学习**：没有显式的训练过程，在训练阶段仅仅是把样本保存起来，训练事件开销为零，待收到测试杨版本后再进行处理

**急切学习**：在训练阶段就对样本进行学习处理的方法

<img src="D:\研究生\研0\machine_learning_notebook\.assets\image-20231121201147626.png" alt="image-20231121201147626" style="zoom:50%;" />

**最近邻分类器（1NN）**在二分类问题上的性能：

给定测试样本$\mathbf{x}$，若其最近邻样本为$\mathbf{z}$，则最近邻分类器出错的概率就是他们两个类别不同的概率，即：
$$
P(err) = 1-\sum_{c\in \mathcal{Y}}P(c|\mathbf{x})P(c|\mathbf{z})
$$
最近邻分类器的泛化错误率不超过**贝叶斯最优分类器**错误率的两倍

### 低纬嵌入

**维数灾难：**在高维情况下出现的数据样本稀疏、距离计算困难等问题

**缓解途径：**降维，或称**维度约简**，即通过某种数学变换将原始高维属性空间转变为一个低维子空间，在这个空间中样本密度大幅提高，距离计算也变得更为容易

**为什么能降维：**在很多时候，人们观测或收集到的数据样本虽是高维的，但与学习任务密切相关的也许仅是某个低维分布，即高维空间中的一个低维**嵌入**

<img src="D:\研究生\研0\machine_learning_notebook\.assets\image-20231121203033955.png" alt="image-20231121203033955" style="zoom: 50%;" />

一种经典的降维方法：**多维缩放**，要求原始空间中样本之间的距离在低维空间中得以保持

<img src="D:\研究生\研0\machine_learning_notebook\.assets\image-20231121211350974.png" alt="image-20231121211350974" style="zoom:50%;" />

一般来说，欲获得低维子空间，最简单的是对原始高维空间进行线性变换，给定d维空间中的样本$\mathbf{X}=(x_1,x_2,...,x_m)\in \R^{d\times m}$，变换之后得到$d^{'}\le d$维空间中的样本：
$$
\mathbf{Z}=\mathbf{W}^T\mathbf{X}\\
其中\mathbf{W}\in \R^{d\times d^{'}}是变换矩阵，\mathbf{Z}\in \R^{d^{'}\times m}是样本在新空间中的表达
$$

### 主成分分析

主成分分析PCA是最常用的一种降维方法

对于正交属性空间中的样本点，如何用一个超平面（直线的高维推广）对所有样本进行恰当的表达？

容易想到，若存在这样的超平面，那么它大概具有这样的性质：

* 最近重构性：样本点到这个超平面的距离都足够近
* 最大可分性：样本点在这个超平面上的投影能尽可能分开

<img src="D:\研究生\研0\machine_learning_notebook\.assets\image-20231121213834071.png" alt="image-20231121213834071" style="zoom:50%;" />

PCA算法描述：

<img src="D:\研究生\研0\machine_learning_notebook\.assets\image-20231121213849496.png" alt="image-20231121213849496" style="zoom: 67%;" />

### 核化线性降维

pass

### 流形学习

### 度量学习

pass

## Chapter7 贝叶斯分类器

### 贝叶斯决策论

> 概率框架下试试决策的基本方法，理想情况下所有概率已知，因此要考虑如何基于这些概率和五哦按损失来选择最优的类别标记

基于后验概率的期望损失为：
$$
R(c_i|\mathbf{x})=\sum_{j=1}^{N}\lambda_{ij}P(c_j|\mathbf{x})
$$
要找一个判定标准h，最小化总体风险：
$$
R(h)=\mathbb{E}_x[R(h(\mathbf{x})|\mathbf{x})]
$$
为最小化总体风险，只需要在每个样本上选择能使条件风险$R(c|\mathbf{x})$最小的类别标记：
$$
h^*(\mathbf{x})=\mathop{arg\ min}_{c\in \mathcal{Y}}R(c|\mathbf{x})\ \ \ \ \ (1)
$$
考虑生成式模型：
$$
P(c|\mathbf{x})=\cfrac{P(\mathbf{x},c)}{P(\mathbf{x})}=\cfrac{P(c)P(\mathbf{x|c})}{P(\mathbf{x})}\ \ \ \ \ (2)
$$

### 极大似然估计

估计类条件概率$P(\mathbf{x}|c)$：先假定其具有某种确定的概率分布形式，再基于训练样本对概率分布的参数进行估计。概率模型的训练过程就是参数估计。

* 频率主义学派：通过优化似然函数来确定参数值，**极大似然估计**
* 贝叶斯学派：参数是未观察到的随机变量，其本身也可有分布

令$D_C$表示训练集D中第c类样本组成的集合，假设这些样本是独立同分布的，则参数$\boldsymbol{\theta}_c$对于数据集$D_c$的似然是：
$$
P(D_c|\boldsymbol{\theta}_c)=\prod_{\mathbf{x}\in D_c}P(\mathbf{x}|\boldsymbol{\theta}_c)
$$
极大似然估计就是试图在$\boldsymbol{\theta}_c$所有可能取值中，找到一个能使数据出现的”可能性“最大的值。

连乘操作易造成下溢，通常使用对数似然：
$$
LL(\boldsymbol{\theta}_c)=\log\ P(D_c|\boldsymbol{\theta}_c) \\
=\sum_{\mathbf{x}\in D_c} \log P(\mathbf{x}|\boldsymbol{\theta_c})
$$
**潜在风险：**这种参数化的方法使类条件概率估计变得相对简单，但是结果的准确性严重依赖于所假设的概率分布形式是否符合潜在的真实数据分布，在现实应用中，要想较好地做出假设，往往需要在一定程度上利用关于任务本身的**经验知识**。

### 朴素贝叶斯分类器

**属性条件独立性假设：**对已知类别，假设所有属性相互独立，即：每个属性独立地对分布结果发生影响。

式（2）可重写为：
$$
P(c|\mathbf{x})=\cfrac{P(\mathbf{x},c)}{P(\mathbf{x})}=\cfrac{P(c)P(\mathbf{x|c})}{P(\mathbf{x})}\\
=\cfrac{P(c)}{p(\mathbf{x})}\prod_{i=1}^dP(x_i|c)
$$
由于对所有类别来说$P(\mathbf{x})$相同，因此基于式（1）的贝叶斯判定准则有：
$$
h_{nb}(\mathbf{x})=\mathop{arg\ max}_{c\in \mathcal{Y}}\ P(c)\prod_{i=1}^dP(x_i|c)
$$
现实任务中朴素贝叶斯分类器有多种使用方式：

* 认为你对预测速度要求较高，实现将朴素贝叶斯分类器所涉及的所有概率估值实现计算好存储起来，在进行预测的时候只需**查表**即可
* 任务数据更替频繁，采用**懒惰学习**，先不进行任务训练，待收到预测请求时再根据当前数据集进行概率估值
* 数据不断增加，可在估值基础上，仅对新增样本的属性值所涉及的概率估值进行计数修正实现增量学习

### 半朴素把贝叶斯分类器

现实任务中**属性条件独立性假设**往往很难成立，对该假设进行一定程度的放松，形成**半朴素贝叶斯分类器**

**独依赖估计：**假设每个属性在类别植物i最多仅依赖于一个其他属性：
$$
P(c|\mathbf{x})\propto P(c)\prod_{i=1}^d P(x_i|c,pa_i)
$$
问题的关键在于如何确定每个属性的父属性$pa_i$：

* 假设所有属性都依赖于同一个属性：**超父**，SPODE方法
* TAN：基于最大带权生成树
* AODE：基于集成学习，尝试将每个属性作为超父来构建SPODE，然后将具有足够训练数据支撑的SPODE集成起来作为最终结果

### 贝叶斯网

也称**信念网**，借助有向无环图来刻画属性之间的依赖关系，并使用条件概率表来描述属性的联合概率分布

一个贝叶斯网由结构G和参数$\Theta$两部分构成，即$B=<G,\Theta>$

<img src=".assets/image-20231117121114089.png" alt="image-20231117121114089" style="zoom: 33%;" />

### EM算法

训练样本并非是完整的，$\mathbf{Z}$是隐变量集，要最大化对数似然：
$$
LL(\Theta|\mathbf{X,Z})= \ln P(\mathbf{X,Z}|\Theta)
$$
因为Z是隐变量，无法直接求解，此时可通过对Z计算期望，来最大化一观测数据的对数**边际似然**
$$
LL(\Theta|\mathbf{X})=\ln P(\mathbf{X}|\Theta)=\ln \sum_{\mathbf{Z}}P(\mathbf{X,Z|\Theta})
$$
EM算法是常用的估计参数隐变量的利器，它是一种迭代式的算法：若参数$\Theta$已定，则可根据训练数据推断出最优隐变量$\mathbf{Z}$的值（E步），反之，若$\mathbf{Z}$的值已知，则可方便地对参数$\Theta$做极大似然估计（M步）
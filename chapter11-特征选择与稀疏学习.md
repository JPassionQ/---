## Chapter11 特征选择与稀疏学习

### 子集搜索与评价

> 对一个学习任务来说，给定属性集，其中有些属性可能很关键、很有用，另一些属性可能没什么用，我们将属性称为**特征**

**相关特征：**对当前学习任务有用的属性，对应**无关特征**

**特征选择：**从给定的特征集合中选择出相关特征

**为什么要进行维数选择？**

* 在现实任务中经常遇到维数灾难问题，此类问题主要由属性过多造成
* 去除不相关特性往往会降低学习的难度

**注意，特征选择中所谓的无关特征是指与当前学习任务无关**

#### 子集搜索

如果没有任何领域知识作为先验假设，就需要遍历所有可能的子集，但是这在计算上是不可行的，因为会遇到组合爆炸的问题，可行的做法是产生一个**候选子集**，评价其好坏，基于评价结果产生下一个候选子集，再对其进行评价，这个过程持续进行下去，直至无法找到更好的候选子集为止。具体策略有：

* 前向搜索
* 后向搜素
* 双向搜索

以上策略均基于**贪心**

#### 子集评价

对每个候选特征子集，我们可基于训练数据集D来计算其信息增益，以此作为评价准则

#### summery

将特征子集搜索机制与子集评价机制相结合，即可得到特征选择方法。例如将前向搜索与信息熵相结合，这显然与决策树算法非常相似。事实上，决策树可用于特征选择，树结点的划分属性所组成的集合就是选择出的特征子集。其他的特征选择方法未必像决策树特征选择这么明显，但它们在本质上都是显式或隐式地结合了某种或多种子集搜索评价机制

### 过滤式选择

> 过滤式方法先对数据集进行特征选择，然后再训练学习器，特征选择过程与后续学习器无关。这相当于先用特征选择过程对初始特征进行**过滤**，再用过滤后的特征来训练模型

Relief是一种著名的过滤式特征选择方法，该方法设计了一个**相关统计量**来度量特征的重要性。

该统计量是一个向量，其每个分量分别对应于一个初始特征，而特征子集的重要性则是由子集中每个特征所对应的相关统计量分量之和来决定。Relief的关键是如何确定相关统计量

给定训练集$\{(\mathbf{x}_1,y_1),(\mathbf{x}_2,y_2),...,(\mathbf{x}_m,y_m)\}$，对每个示例$\mathbf{x}_i$，Relief先在$\mathbf{x}_i$的同类样本中寻找其最近邻$\mathbf{x}_{i,nh}$，称为**猜中近邻（near-hit）**，再从$\mathbf{x}_i$的异类样本中寻找其最近邻$\mathbf{x}_{i,nm}$称为**猜错近邻（near-miss）**，然后，相关统计量对应于属性j的分量为：
$$
\delta^j=\sum_i-diff(x_i^j,x_{i,nh}^j)^2+diff(x_i^j,x_{i,nm}^j)^2\ \ \ \ (1)
$$
其中$x_a^j$表示样本$\mathbf{x}_a$在属性j上的取值，$diff(x_a^j,x_b^j)$取决于属性j的类型：

* 若属性j为离散型，则$x_a^j=x_b^j时diff(x_a^j,x_b^j)=0$，否则为1
* 若属性j为连续性，则$diff(x_a^j,x_b^j)=|x_a^j-x_b^j|$，注意$x_a^j，x_b^j$已经规范化到[0, 1]区间

从式（1）可以看出，若$\mathbf{x}_i$与其猜中近邻$\mathbf{x}_{i,nh}$在属性j上的距离小于$\mathbf{x}_i$与其猜错近邻$\mathbf{x}_{i,nm}$的距离，则说明属性j对区分同类与异类样本是有益的，于是增大属性j所对应的统计量分量；反之说明属性j其负面作用，于是减小属性j所对应的统计量分量。最后，对基于不同样本得到得到估计结果进行平均，就得到个属性的相关统计量分量，分量值越大，则对应属性的分类能力就越强
